{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":39911,"sourceType":"datasetVersion","datasetId":31296},{"sourceId":8218643,"sourceType":"datasetVersion","datasetId":4871738},{"sourceId":8244576,"sourceType":"datasetVersion","datasetId":4891178},{"sourceId":8643273,"sourceType":"datasetVersion","datasetId":5176505}],"dockerImageVersionId":30185,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# For ML Models\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras.layers import *\nfrom tensorflow.keras.models import *\nfrom tensorflow.keras.losses import *\nfrom tensorflow.keras.optimizers import *\nfrom tensorflow.keras.metrics import *\nfrom tensorflow.keras import regularizers\nimport tensorflow.keras.backend as K\nfrom tensorflow.keras.preprocessing.image import load_img\n\n# For Data Processing\nimport numpy as np\n\n# For Data Visualization\nimport matplotlib.pyplot as plt\n\n# Miscellaneous\nimport os\nimport random\n\n# Turn off warnings\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' ","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_kg_hide-output":true,"execution":{"iopub.status.busy":"2024-07-01T22:02:18.423551Z","iopub.execute_input":"2024-07-01T22:02:18.423910Z","iopub.status.idle":"2024-07-01T22:02:24.588746Z","shell.execute_reply.started":"2024-07-01T22:02:18.423815Z","shell.execute_reply":"2024-07-01T22:02:24.587979Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"TRAIN_SIZE = 384\nINFERENCE_SIZE = 224","metadata":{"execution":{"iopub.status.busy":"2024-06-08T19:17:58.947940Z","iopub.execute_input":"2024-06-08T19:17:58.948543Z","iopub.status.idle":"2024-06-08T19:17:58.952202Z","shell.execute_reply.started":"2024-06-08T19:17:58.948506Z","shell.execute_reply":"2024-06-08T19:17:58.951462Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"main_dir = '/kaggle/input/flickr-image-dataset/flickr30k_images/flickr30k_images/'\nall_image_paths = [main_dir+file for file in os.listdir(main_dir) if file.endswith('.jpg')]\n\nprint('Total number of images:', len(all_image_paths))","metadata":{"execution":{"iopub.status.busy":"2024-06-08T19:18:01.002464Z","iopub.execute_input":"2024-06-08T19:18:01.003075Z","iopub.status.idle":"2024-06-08T19:18:01.403182Z","shell.execute_reply.started":"2024-06-08T19:18:01.003042Z","shell.execute_reply":"2024-06-08T19:18:01.402343Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_image_paths = all_image_paths[:25000]\n\nremaining_paths = all_image_paths[25000:]\n\nnum_remaining = len(remaining_paths)\nnum_valid = num_remaining // 2\nnum_test = num_remaining - num_valid  # Acest lucru asigură că toate imaginile rămase sunt utilizate\n\nvalid_image_paths = remaining_paths[:num_valid]\ntest_image_paths = remaining_paths[num_valid:]","metadata":{"execution":{"iopub.status.busy":"2024-06-08T19:18:04.268122Z","iopub.execute_input":"2024-06-08T19:18:04.268850Z","iopub.status.idle":"2024-06-08T19:18:04.272849Z","shell.execute_reply.started":"2024-06-08T19:18:04.268812Z","shell.execute_reply":"2024-06-08T19:18:04.272103Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def open_images(paths, size=TRAIN_SIZE):\n    '''\n    Given an array of paths to images, this function opens those images,\n    and returns them as an array of shape (None, Height, Width, Channels)\n    '''\n    images = []\n    for path in paths:\n        image = load_img(path, target_size=(size, size, 3))\n        image = np.array(image)/255.0 # Normalize image pixel values to be between 0 and 1\n        images.append(image)\n    return np.array(images)","metadata":{"execution":{"iopub.status.busy":"2024-06-08T19:18:06.912452Z","iopub.execute_input":"2024-06-08T19:18:06.912817Z","iopub.status.idle":"2024-06-08T19:18:06.919635Z","shell.execute_reply.started":"2024-06-08T19:18:06.912776Z","shell.execute_reply":"2024-06-08T19:18:06.918820Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def add_noise(images, amount=0.1):\n    '''\n    Given an array of images [a shape of (None, Height, Width, Channels)],\n    this function adds gaussian noise to every channel of the images\n    '''\n    # Create a matrix with values with a mean of 0 and standard deviation of \"amount\"\n    noise = np.random.normal(0, amount, images.shape[0]*images.shape[1]*images.shape[2]*images.shape[3]).reshape(images.shape)\n    # Add noise to images\n    noise_img = images+noise\n    return noise_img","metadata":{"execution":{"iopub.status.busy":"2024-06-08T19:18:11.030124Z","iopub.execute_input":"2024-06-08T19:18:11.030758Z","iopub.status.idle":"2024-06-08T19:18:11.036031Z","shell.execute_reply.started":"2024-06-08T19:18:11.030720Z","shell.execute_reply":"2024-06-08T19:18:11.035171Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"image = open_images([train_image_paths[400]])\nnoise_img = add_noise(image, amount=0.2)\n\nfig = plt.figure(figsize=(10, 5))\n# Plot Image\nfig.add_subplot(1, 2, 1)\nplt.axis('off')\nplt.title('Image')\nplt.imshow(image[0])\nfig.add_subplot(1, 2, 2)\nplt.axis('off')\nplt.title('Image with noise')\nplt.imshow(noise_img[0])\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-06-08T19:18:13.540203Z","iopub.execute_input":"2024-06-08T19:18:13.541012Z","iopub.status.idle":"2024-06-08T19:18:13.919914Z","shell.execute_reply.started":"2024-06-08T19:18:13.540970Z","shell.execute_reply":"2024-06-08T19:18:13.919156Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def datagen(paths, size=TRAIN_SIZE, batch_size=5, is_training=True):\n \n    for x in range(0, len(paths), batch_size):\n        batch_paths = paths[x:x+batch_size]\n        batch_images = open_images(batch_paths, size=size)\n        if is_training:\n            amount = random.uniform(0, 0.2)  # Amount of noise = random value between 0 and 0.2\n            noise_images = add_noise(batch_images, amount=amount)\n        else:\n            noise_images = batch_images  # No noise added for validation data\n        yield noise_images, batch_images","metadata":{"execution":{"iopub.status.busy":"2024-06-08T19:18:20.600662Z","iopub.execute_input":"2024-06-08T19:18:20.601267Z","iopub.status.idle":"2024-06-08T19:18:20.606957Z","shell.execute_reply.started":"2024-06-08T19:18:20.601235Z","shell.execute_reply":"2024-06-08T19:18:20.606194Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"image = Input(shape=(None,None,3))\n\n# encoder\nl1 = Conv2D(64, (3,3), padding='same', activation='relu')(image)     \nl2 = Conv2D(64, (3,3), padding='same', activation='relu')(l1)\n\nl3 = MaxPooling2D(padding='same')(l2)\nl3 = Dropout(0.3)(l3)\nl4 = Conv2D(128, (3,3), padding='same', activation='relu')(l3)\nl5 = Conv2D(128, (3,3), padding='same', activation='relu')(l4)\n\nl6 = MaxPooling2D(padding='same')(l5)\nl7 = Conv2D(256, (3,3), padding='same', activation='relu',)(l6)\n\n#decoder\nl8 = UpSampling2D()(l7)\nl9 = Conv2D(128, (3,3), padding='same', activation='relu')(l8)\nl10 = Conv2D(128, (3,3), padding='same', activation='relu')(l9)\n\nl11 = add([l5,l10])\nl12 = UpSampling2D()(l11)\nl13 = Conv2D(64, (3,3), padding='same', activation='relu')(l12)\nl14 = Conv2D(64, (3,3), padding='same', activation='relu')(l13)\n\nl15 = add([l14,l2])\n\ndecoded = Conv2D(3, (3,3), padding='same', activation='relu')(l15)\nmodel = Model(image, decoded)","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2024-06-08T19:18:25.844313Z","iopub.execute_input":"2024-06-08T19:18:25.844830Z","iopub.status.idle":"2024-06-08T19:18:28.683752Z","shell.execute_reply.started":"2024-06-08T19:18:25.844795Z","shell.execute_reply":"2024-06-08T19:18:28.682754Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.summary()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2024-05-31T12:10:27.956014Z","iopub.execute_input":"2024-05-31T12:10:27.956290Z","iopub.status.idle":"2024-05-31T12:10:27.982342Z","shell.execute_reply.started":"2024-05-31T12:10:27.956260Z","shell.execute_reply":"2024-05-31T12:10:27.981107Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tf.keras.utils.plot_model(model, show_shapes=True, show_layer_names=False, dpi=70)","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2024-05-31T12:15:25.204434Z","iopub.execute_input":"2024-05-31T12:15:25.205226Z","iopub.status.idle":"2024-05-31T12:15:26.092326Z","shell.execute_reply.started":"2024-05-31T12:15:25.205188Z","shell.execute_reply":"2024-05-31T12:15:26.091427Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# model.compile(optimizer='adam',\n#               loss='mean_squared_error',\n#               metrics=['mean_squared_error'])\n\nlearning_rate = 0.00001  \noptimizer = Adam(learning_rate=learning_rate)\n\nmodel.compile(optimizer=optimizer,\n              loss='mean_squared_error')","metadata":{"execution":{"iopub.status.busy":"2024-06-08T19:18:43.641894Z","iopub.execute_input":"2024-06-08T19:18:43.642613Z","iopub.status.idle":"2024-06-08T19:18:43.657357Z","shell.execute_reply.started":"2024-06-08T19:18:43.642574Z","shell.execute_reply":"2024-06-08T19:18:43.656675Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.models import load_model\n\n# Înlocuiește acesta cu calea la modelul specific pe care dorești să-l încarci\nmodel_path = '/kaggle/input/model-nou/autoencoder_model_epoch_5.h5'\nmodel = load_model(model_path)\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"batch_size=10\nsteps = int(len(train_image_paths)/batch_size)\nvalid_steps = len(valid_image_paths) // batch_size\n# Liste pentru stocarea istoricului de loss\ntrain_losses = []\nval_losses = []\nepochs = 5\n\n# Antrenează modelul și acumulează istoricul\nfor epoch in range(epochs):\n    history = model.fit(\n        datagen(train_image_paths, size=TRAIN_SIZE, batch_size=batch_size, is_training=True),\n        validation_data=datagen(valid_image_paths, size=TRAIN_SIZE, batch_size=batch_size),\n        epochs=1,\n        steps_per_epoch=steps,\n        validation_steps=valid_steps\n    )\n    train_losses.extend(history.history['loss'])\n    val_losses.extend(history.history['val_loss'])\n\n    model.save(f'/kaggle/working/autoencoder_model_epoch_{epoch+6}.h5')\n","metadata":{"execution":{"iopub.status.busy":"2022-04-28T13:44:34.610488Z","iopub.execute_input":"2022-04-28T13:44:34.611052Z","iopub.status.idle":"2022-04-28T13:59:52.920621Z","shell.execute_reply.started":"2022-04-28T13:44:34.611011Z","shell.execute_reply":"2022-04-28T13:59:52.918231Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\n# Creează o listă cu numărul de epoci pentru axa x\nepochs_range = range(1, len(train_losses) + 1)\n\n# Plotare\nplt.figure(figsize=(8, 5))\nplt.plot(epochs_range, train_losses, label='Training Loss')\nplt.plot(epochs_range, val_losses, label='Validation Loss')\nplt.title('Training and Validation Loss')\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.legend()\nplt.show()\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"batch_size = 10\nsteps = int(len(test_image_paths) / batch_size)\nloss = model.evaluate(datagen(test_image_paths, size=INFERENCE_SIZE, batch_size=batch_size), steps=steps)\nprint(\"Loss on test set:\", loss)","metadata":{"execution":{"iopub.status.busy":"2022-04-28T13:34:53.350538Z","iopub.execute_input":"2022-04-28T13:34:53.351177Z","iopub.status.idle":"2022-04-28T13:36:52.329906Z","shell.execute_reply.started":"2022-04-28T13:34:53.35114Z","shell.execute_reply":"2022-04-28T13:36:52.329126Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def plot_results(noise_image, reconstructed_image, image):\n    w = 15\n    h = len(noise_image)*5\n    fig = plt.figure(figsize=(w, h))\n    columns = 3\n    rows = len(noise_image)\n    for i in range(1, rows*columns, columns):\n        fig.add_subplot(rows, columns, i)\n        plt.axis('off')\n        plt.title('Image with noise')\n        plt.imshow(noise_images[int((i-1)/columns)])\n    \n        fig.add_subplot(rows, columns, i+1)\n        plt.axis('off')\n        plt.title('Reconstructed Image')\n        plt.imshow(reconstructed[int((i-1)/columns)])\n        \n        fig.add_subplot(rows, columns, i+2)\n        plt.axis('off')\n        plt.title('Original Image')\n        plt.imshow(images[int((i-1)/columns)])\n    \n    plt.show()\n    \nbatch_size = 7\n\npaths = random.sample(test_image_paths, batch_size)\nimages = open_images(paths, size=INFERENCE_SIZE)\n# Amount of noise = random value between 0.1 and 0.15\namount = random.uniform(0.1,0.15)\nnoise_images = add_noise(images, amount=amount)\nreconstructed = model.predict(noise_images)\n\nplot_results(noise_images, reconstructed, images)","metadata":{"execution":{"iopub.status.busy":"2022-04-29T11:17:45.481773Z","iopub.execute_input":"2022-04-29T11:17:45.484015Z","iopub.status.idle":"2022-04-29T11:17:47.313456Z","shell.execute_reply.started":"2022-04-29T11:17:45.483975Z","shell.execute_reply":"2022-04-29T11:17:47.312592Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nfrom tensorflow.keras.preprocessing.image import load_img, img_to_array, save_img\nimport matplotlib.pyplot as plt\nfrom tensorflow.keras.models import load_model\nimport tensorflow as tf\n\nmodel = load_model('/kaggle/input/denoise-model/autoencoder_model (5).h5')\nTRAIN_SIZE = 384\n\ndef denoise_img(image_path, save_path=None):\n    \"\"\"\n    This function takes an image path, denoises the image using the trained model,\n    saves the denoised image to a specified path, and displays the original and denoised images.\n    \n    :param image_path: path to the input image\n    :param save_path: path to save the denoised image (if None, it won't save)\n    \"\"\"\n    # Load and preprocess the image\n    original_img = load_img(image_path)\n    img = load_img(image_path, target_size=(TRAIN_SIZE, TRAIN_SIZE))  # Resize image for the model\n    img_array = img_to_array(img) / 255.0  # Normalize the image array\n    img_array = np.expand_dims(img_array, axis=0)  # Expand dims to fit model input\n\n    # Predict the denoised image\n    denoised_img_array = model.predict(img_array)[0]\n    \n    # Rescale the pixels values\n    denoised_img_array = np.clip(denoised_img_array, 0, 1)\n    \n    # Resize back to original dimensions\n    denoised_img = tf.image.resize(denoised_img_array, (original_img.size[1], original_img.size[0]))\n    denoised_img = np.clip(denoised_img.numpy(), 0, 1)  # Convert to numpy and clip any potential rounding discrepancies\n\n    # Save the denoised image if a save_path is provided\n    if save_path:\n        save_img(save_path, denoised_img * 255)  # Multiply by 255 to convert back to original scale\n\n    # Display the original and the denoised images\n    plt.figure(figsize=(12, 6))\n    plt.subplot(1, 2, 1)\n    plt.imshow(original_img)\n    plt.title('Original Image')\n    plt.axis('off')\n\n    plt.subplot(1, 2, 2)\n    plt.imshow(denoised_img)\n    plt.title('Denoised Image')\n    plt.axis('off')\n\n    plt.show()\n\nimage_path = '/kaggle/input/poza-caine-pisica/istockphoto-489272417-612x612.jpg'\nsave_path = '/kaggle/working/denoise.jpg'\ndenoise_img(image_path, save_path)","metadata":{"execution":{"iopub.status.busy":"2024-05-31T22:48:23.253627Z","iopub.execute_input":"2024-05-31T22:48:23.254011Z","iopub.status.idle":"2024-05-31T22:48:39.963614Z","shell.execute_reply.started":"2024-05-31T22:48:23.253908Z","shell.execute_reply":"2024-05-31T22:48:39.962668Z"},"trusted":true},"execution_count":null,"outputs":[]}]}