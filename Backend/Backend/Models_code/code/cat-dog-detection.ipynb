{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.9","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":1151625,"sourceType":"datasetVersion","datasetId":650614},{"sourceId":5488847,"sourceType":"datasetVersion","datasetId":3166942},{"sourceId":8292876,"sourceType":"datasetVersion","datasetId":4926555},{"sourceId":8322496,"sourceType":"datasetVersion","datasetId":4943794},{"sourceId":8327375,"sourceType":"datasetVersion","datasetId":4945539},{"sourceId":8650793,"sourceType":"datasetVersion","datasetId":5181784}],"dockerImageVersionId":30086,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport pandas as pd \nimport os\nimport matplotlib.pyplot as plt\nimport cv2\nfrom tqdm import tqdm\n\nimport random\nimport torch\nfrom torch.utils.data import DataLoader, Dataset\nfrom torch.utils.data.sampler import SequentialSampler\nimport xml.etree.ElementTree as ET\n\nimport torchvision\nfrom torchvision.models.detection.faster_rcnn import FastRCNNPredictor\nfrom torchvision.models.detection import FasterRCNN\nfrom torchvision.models.detection.rpn import AnchorGenerator\n\nfrom PIL import Image\n\nimport albumentations as A \nfrom albumentations.pytorch.transforms import ToTensorV2","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-06-12T00:28:05.574275Z","iopub.execute_input":"2024-06-12T00:28:05.574697Z","iopub.status.idle":"2024-06-12T00:28:08.810712Z","shell.execute_reply.started":"2024-06-12T00:28:05.574574Z","shell.execute_reply":"2024-06-12T00:28:08.809750Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"def set_seeds(seed=42):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    torch.backends.cudnn.deterministic = True\n\nset_seeds();","metadata":{"execution":{"iopub.status.busy":"2024-06-09T23:45:59.679676Z","iopub.execute_input":"2024-06-09T23:45:59.680060Z","iopub.status.idle":"2024-06-09T23:45:59.689239Z","shell.execute_reply.started":"2024-06-09T23:45:59.680021Z","shell.execute_reply":"2024-06-09T23:45:59.688532Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\ntorch.cuda.empty_cache()","metadata":{"execution":{"iopub.status.busy":"2024-06-09T23:45:59.690445Z","iopub.execute_input":"2024-06-09T23:45:59.690723Z","iopub.status.idle":"2024-06-09T23:45:59.695142Z","shell.execute_reply.started":"2024-06-09T23:45:59.690696Z","shell.execute_reply":"2024-06-09T23:45:59.694357Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport shutil\nimport csv\nfrom random import sample\nimport xml.etree.ElementTree as ET\nimport cv2  # For reading image dimensions\n\ndef copy_images_and_create_xml(input_folder, dest_folder, annotation_csv, annotation_dest_folder, num_images=1300):\n    # Ensure the destination folders exist\n    os.makedirs(dest_folder, exist_ok=True)\n    os.makedirs(annotation_dest_folder, exist_ok=True)\n\n    # Load annotations from CSV into a dictionary\n    annotations = {}\n    with open(annotation_csv, newline='') as csvfile:\n        reader = csv.DictReader(csvfile)\n        for row in reader:\n            annotations[row['image']] = row\n\n    # Get all files in the input folder that start with \"CAT_CAT\"\n    all_files = [f for f in os.listdir(input_folder) if os.path.isfile(os.path.join(input_folder, f)) and f.startswith(\"CAT_CAT\")]\n    \n    # Randomly select 'num_images' from the list of files, ensuring we don't exceed the number available\n    selected_files = sample(all_files, min(num_images, len(all_files)))\n\n    # Copy each selected file to the destination folder and create XML\n    for file_name in selected_files:\n        if file_name in annotations:\n            source_path = os.path.join(input_folder, file_name)\n            img = cv2.imread(source_path)  # Read the image to get its dimensions\n            if img is not None:\n                img_height, img_width = img.shape[:2]\n                if is_bounding_box_valid(annotations[file_name], img_width, img_height):\n                    destination_path = os.path.join(dest_folder, file_name)\n                    shutil.copy(source_path, destination_path)\n                    create_xml_file(file_name, annotations[file_name], destination_path, annotation_dest_folder)\n                else:\n                    print(\"not valid bb\")\n\ndef is_bounding_box_valid(annotation_data, img_width, img_height):\n    xmin = float(annotation_data['x'])\n    ymin = float(annotation_data['y'])\n    xmax = xmin + float(annotation_data['width'])\n    ymax = ymin + float(annotation_data['height'])\n    # Normalize coordinates\n    xmin /= img_width\n    xmax /= img_width\n    ymin /= img_height\n    ymax /= img_height\n    # Check if bounding box is within the image bounds\n    return 0 <= xmin < xmax <= 1 and 0 <= ymin < ymax <= 1\n\ndef create_xml_file(image_name, annotation_data, image_path, dest_folder):\n    base_name = os.path.splitext(image_name)[0]\n    \n    xmin = int(float(annotation_data['x']))\n    ymin = int(float(annotation_data['y']))\n    xmax = xmin + int(float(annotation_data['width']))\n    ymax = ymin + int(float(annotation_data['height']))\n    \n    root = ET.Element(\"annotation\")\n    ET.SubElement(root, \"filename\").text = image_name\n    ET.SubElement(root, \"path\").text = image_path\n    obj = ET.SubElement(root, \"object\")\n    ET.SubElement(obj, \"name\").text = \"cat\"\n    bndbox = ET.SubElement(obj, \"bndbox\")\n    ET.SubElement(bndbox, \"xmin\").text = str(xmin)\n    ET.SubElement(bndbox, \"ymin\").text = str(ymin)\n    ET.SubElement(bndbox, \"xmax\").text = str(xmax)\n    ET.SubElement(bndbox, \"ymax\").text = str(ymax)\n    \n    tree = ET.ElementTree(root)\n    xml_file_path = os.path.join(dest_folder, f\"{base_name}.xml\")\n    tree.write(xml_file_path)\n\n# Example usage\ninput_folder = '/kaggle/input/cat-faces-detection/result-dataset/result-dataset/data'\ndest_folder = '/kaggle/working/imgs/'\nannotation_csv = '/kaggle/input/cat-faces-detection/result-dataset/result-dataset/info.csv'\nannotation_dest_folder = '/kaggle/working/anno/'\ncopy_images_and_create_xml(input_folder, dest_folder, annotation_csv, annotation_dest_folder)\n","metadata":{"execution":{"iopub.status.busy":"2024-06-09T23:45:59.696452Z","iopub.execute_input":"2024-06-09T23:45:59.696787Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"IMG_DIR = '/kaggle/input/dog-and-cat-detection/images'\nANN_DIR = '/kaggle/input/dog-and-cat-detection/annotations'\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def extract_annotations(annotation_path):\n    root = ET.parse(annotation_path).getroot()\n    class_name = root.find(\"./object/name\").text\n    xmin = int(root.find(\"./object/bndbox/xmin\").text)\n    ymin = int(root.find(\"./object/bndbox/ymin\").text)\n    xmax = int(root.find(\"./object/bndbox/xmax\").text)\n    ymax = int(root.find(\"./object/bndbox/ymax\").text)\n    return [ymin, xmin, ymax, xmax], class_name","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport xml.etree.ElementTree as ET\n\ndef count_labels_in_folder(folder_path):\n    cat_count = 0\n    dog_count = 0\n\n    # Iterate through each file in the folder\n    for file_name in os.listdir(folder_path):\n        if file_name.endswith(\".xml\"):\n            # Extract annotations from XML file\n            annotation_path = os.path.join(folder_path, file_name)\n            _, class_name = extract_annotations(annotation_path)\n\n            # Count occurrences of labels \"cat\" and \"dog\"\n            if class_name == \"cat\":\n                cat_count += 1\n            elif class_name == \"dog\":\n                dog_count += 1\n\n    return cat_count, dog_count\n\n# Usage example\nfolder_path = \"/kaggle/input/dog-and-cat-detection/annotations\"\ncat_count, dog_count = count_labels_in_folder(folder_path)\nprint(\"Number of cats:\", cat_count)\nprint(\"Number of dogs:\", dog_count)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# def display_boxes(image_path, boxes, labels):\n#     image = cv2.imread(image_path)\n#     image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)    \n#     fig, ax = plt.subplots(1, 1, figsize=(16, 8))\n#     ax.set_axis_off()\n\n#     alpha_box = 0.4\n#     text_height = 25\n#     text_width = 20\n#     color = (0, 255, 0)\n\n#     for box, label in zip(boxes, labels):\n#         output = image.copy()\n#         output = cv2.rectangle(image, (int(box[0]), int(box[1])), (int(box[2]), int(box[3])), color, 2)\n\n#         overlay_text = image.copy()\n\n#         cv2.rectangle(overlay_text, (box[0], box[1]-7-text_height),\n#                       (box[0]+text_width+2, box[1]), color, -1)\n\n#         cv2.addWeighted(overlay_text, alpha_box, output, 1 - alpha_box, 0, output)\n#         cv2.putText(output, str(label), (box[0], box[1]-5),\n#                 cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 0), 2, cv2.LINE_AA)\n\n#     plt.imshow(image) \n#     print(image.shape)   \n\n\nimport cv2\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as patches\n\ndef display_boxes(image_path, bb, class_name):\n    # Load the image\n    image = cv2.imread(image_path)\n    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)  # Convert from BGR to RGB\n\n    # Create figure and axes\n    fig, ax = plt.subplots()\n\n    # Display the image\n    ax.imshow(image)\n\n    # Create a Rectangle patch\n    rect = patches.Rectangle((bb[1], bb[0]), bb[3] - bb[1], bb[2] - bb[0], linewidth=2, edgecolor='r', facecolor='none')\n\n    # Add the rectangle to the Axes\n    ax.add_patch(rect)\n\n    # Add label\n    plt.text(bb[1], bb[0] - 10, class_name, color='red', fontsize=15)\n\n    # Remove axis\n    plt.axis('off')\n\n    # Show the plot\n    plt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # image_id = 'i.rf.f6f93b07ea662b0526a5bc10b5ff2677'\n# # image_path = os.path.join(IMG_DIR, (image_id + '.jpg'))\n# # image = cv2.imread(image_path)\n# # image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) \n# # boxes = df.loc[df['image_id'] == image_id, ['x_min', 'y_min', 'x_max', 'y_max']].values\n# # labels = df.loc[df['image_id'] == image_id, 'class'].values\n\n# img_path = '/kaggle/working/imgs/CAT_CAT_06-00001489_015.jpg'\n# bb, class_name =extract_annotations('/kaggle/working/anno/CAT_CAT_06-00001489_015.xml')\n# print(bb)\n# display_boxes(img_path, bb, class_name)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Datasets","metadata":{}},{"cell_type":"code","source":"import torch\ntorch.cuda.empty_cache()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# image_ids = df['image_id'].unique()\n# random.shuffle(image_ids)\n# train_ids = image_ids[:300]\n# valid_ids = image_ids[300:]\n\nadd_img= '/kaggle/working/imgs/'\nimages_list = [image for image in os.listdir(IMG_DIR)]+ [(add_img+ image) for image in os.listdir(add_img)]\nnp.random.shuffle(images_list)\ntrain_image_list = images_list[:int(0.9*len(images_list))]\ntest_image_list = images_list[int(0.9*len(images_list)):]\n# SHUFFLE THE LIST\nnp.random.shuffle(train_image_list)\nnp.random.shuffle(test_image_list)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n\n# train_df = df[df['image_id'].isin(train_ids)]\n# valid_df = df[df['image_id'].isin(valid_ids)]\n# valid_df.shape, train_df.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class CatsDogsDataset(Dataset):\n\n    def __init__(self, image_paths, annotation_dir, transforms=None):\n        super().__init__()\n        self.image_paths = image_paths\n        self.annotation_dir = annotation_dir\n        self.transforms = transforms\n\n    def extract_annotations(self, annotation_path):\n        root = ET.parse(annotation_path).getroot()\n        class_name = root.find(\"./object/name\").text\n        xmin = int(root.find(\"./object/bndbox/xmin\").text)\n        ymin = int(root.find(\"./object/bndbox/ymin\").text)\n        xmax = int(root.find(\"./object/bndbox/xmax\").text)\n        ymax = int(root.find(\"./object/bndbox/ymax\").text)\n        return [xmin, ymin, xmax, ymax], class_name\n\n    def __getitem__(self, index: int):\n        if \"working\" in self.image_paths[index]:\n            image_path = self.image_paths[index]\n            annotation_file_name = (image_path.split('/')[-1]).split('.')[0] + '.xml'\n            annotation_path = os.path.join('/kaggle/working/anno', annotation_file_name)\n\n\n        else:\n            image_path = IMG_DIR+ '/'+self.image_paths[index]\n            annotation_file_name = (image_path.split('/')[-1]).split('.')[0] + '.xml'\n            annotation_path = os.path.join(self.annotation_dir, annotation_file_name)\n\n        image = cv2.imread(image_path, cv2.IMREAD_COLOR)\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB).astype(np.float32)\n        image /= 255.0\n        original_shape = image.shape\n        h, w, _ = image.shape\n  \n\n\n\n        bb, class_name = self.extract_annotations(annotation_path)\n            \n        xminn, yminn, xmaxn, ymaxn = bb\n\n        bb_normalized = [xminn / w, yminn / h, xmaxn / w, ymaxn / h]\n        if any(coord < 0 or coord > 1 for coord in bb_normalized):\n            print(f\"Skipping image {image_path} due to invalid bounding box.\")\n            return None  # or handle the case as needed\n\n\n  \n\n\n        # For multi-label scenarios, class index needs adjustment\n        if(class_name ==\"cat\"):\n            class_index=1\n        else:\n            class_index= 2\n        \n        target = {\n            'boxes': torch.tensor([bb], dtype=torch.float32),\n            'labels': torch.tensor([class_index], dtype=torch.int64),\n            'area': torch.tensor([(bb[2] - bb[0]) * (bb[3] - bb[1])], dtype=torch.float32),\n            'iscrowd': torch.zeros(1, dtype=torch.int64)\n        }\n\n        if self.transforms:\n            sample = {\n                'image': image,\n                'bboxes': target['boxes'],\n                'labels': target['labels']\n            }\n            \n            sample = self.transforms(**sample)\n            image = sample['image']\n            transformed_bbox = sample['bboxes'][0] if sample['bboxes'] else 'No bboxes'\n            \n\n            \n            target['boxes'] = torch.tensor(sample['bboxes']).float() if sample['bboxes'] else torch.zeros((0, 4))\n            \n\n        return image, target\n\n    def __len__(self):\n        return len(self.image_paths)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_train_transform():\n    return A.Compose([\n        A.HorizontalFlip(p=0.5),  # Flip the image horizontally 50% of the time\n#         A.ShiftScaleRotate(shift_limit=0.0625, scale_limit=0.2, rotate_limit=45, p=0.5),  # Randomly apply affine transforms: translate, scale and rotate the image\n        A.RandomBrightnessContrast(p=0.2),  # Randomly change brightness and contrast\n#         A.GaussianBlur(blur_limit=(3, 5), p=0.2),  # Apply Gaussian Blur to 20% of images\n        A.Resize(height=512, width=512, p=1.0),  # Resize all images to 512x512 for consistency\n        ToTensorV2(p=1.0)  # Convert the image and its properties to a PyTorch Tensor\n    ], bbox_params=A.BboxParams(\n    format='pascal_voc', \n    label_fields=['labels'],\n    min_visibility=0.3,  # Adjust this to ignore bounding boxes that become too small or go out of frame\n    min_area=0,  # Consider setting a minimum area to ensure visibility\n))\n\n\n\ndef get_valid_transform():\n    return A.Compose([\n        A.Resize(height=512, width=512, p=1.0),  # Resize all images to 512x512 for consistency\n        ToTensorV2(p=1.0)  # Ensure validation transformations match training for consistent input size\n    ], bbox_params=A.BboxParams(\n    format='pascal_voc', \n    label_fields=['labels'],\n    min_visibility=0.3,  # Adjust this to ignore bounding boxes that become too small or go out of frame\n    min_area=0,  # Consider setting a minimum area to ensure visibility\n))\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\ntorch.cuda.empty_cache()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_dataset = CatsDogsDataset(train_image_list, ANN_DIR, get_train_transform())\n\nvalid_dataset = CatsDogsDataset(test_image_list, ANN_DIR, get_valid_transform())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def collate_fn(batch):\n    return tuple(zip(*batch))\n\ntrain_data_loader = DataLoader(\n    train_dataset,\n    batch_size=10,\n    shuffle=True,\n    collate_fn=collate_fn,\n        num_workers=4  # Adjust based on your system's specification\n\n)\n\nvalid_data_loader = DataLoader(\n    valid_dataset,\n    batch_size=10,\n    shuffle=False,\n    collate_fn=collate_fn,\n        num_workers=4  # Adjust based on your system's specification\n\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model","metadata":{}},{"cell_type":"code","source":"# model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# num_classes = 3  # + background\n\n# # get number of input features for the classifier\n# in_features = model.roi_heads.box_predictor.cls_score.in_features\n\n# # replace the pre-trained head with a new one\n# model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n\n\nimport torchvision\nfrom torchvision.models.detection import FasterRCNN\nfrom torchvision.models.detection.faster_rcnn import FastRCNNPredictor\nfrom torchvision.models.detection.backbone_utils import resnet_fpn_backbone\nimport torch.nn as nn\n\ndef create_custom_model(num_classes, freeze_layers=True):\n    # Load a pre-trained ResNet-50 FPN backbone\n    backbone = resnet_fpn_backbone('resnet50', pretrained=True)\n\n    if freeze_layers:\n        # Freeze the specified layers\n        for name, parameter in backbone.named_parameters():\n            # Freeze all except the last two layers (layer3 and layer4 of ResNet-50)\n            if 'layer3' not in name and 'layer4' not in name:\n                parameter.requires_grad = False\n\n    # Initialize the Faster R-CNN model with the custom backbone\n    model = FasterRCNN(backbone, num_classes=num_classes)\n\n    # Replace the pre-trained head with a new one (necessary due to num_classes change)\n    in_features = model.roi_heads.box_predictor.cls_score.in_features\n    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n\n    # Add custom layers to the model to enhance feature extraction\n    # Adding a simple FC layer sequence after the ROI pooling step\n    additional_layers = nn.Sequential(\n        nn.Linear(in_features, 1024),\n        nn.ReLU(),\n        nn.Dropout(0.5),\n        nn.Linear(1024, 512),\n        nn.ReLU(),\n        nn.Dropout(0.5)\n    )\n    # Append these layers to the existing model.roi_heads\n    model.roi_heads.additional_layers = additional_layers\n\n    return model\n\n# Configure your model\nnum_classes = 3  # 1 background \nmodel = create_custom_model(num_classes)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\ntorch.cuda.empty_cache()\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Training","metadata":{}},{"cell_type":"code","source":"torch.cuda.empty_cache()\n\ndevice = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\nmodel.to(device)\n\nparams = [p for p in model.parameters() if p.requires_grad]\n# optimizer = torch.optim.SGD(params, lr=0.00001, momentum=0.9, weight_decay=0.0005)\noptimizer = torch.optim.Adam(params, lr=0.00001, weight_decay=0.0005)\n\n\nlr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.1)\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Averager:\n    def __init__(self):\n        self.current_total = 0.0\n        self.iterations = 0.0\n\n    def send(self, value):\n        self.current_total += value\n        self.iterations += 1\n\n    @property\n    def value(self):\n        if self.iterations == 0:\n            return 0\n        else:\n            return 1.0 * self.current_total / self.iterations\n\n    def reset(self):\n        self.current_total = 0.0\n        self.iterations = 0.0","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\ntorch.cuda.empty_cache()\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# import matplotlib.pyplot as plt\n\n# EPOCHES = 10\n# PRINT_STEP = 10\n# epoch_losses = []  # List to store average loss per epoch\n\n# loss_hist = Averager()\n# itr = 1\n    \n# for epoch in range(EPOCHES):\n#     loss_hist.reset()\n#     model.train()\n    \n#     for data in train_data_loader:\n#         if data is None:\n#             continue\n        \n#         images, targets = data\n\n#         images = list(image.to(device) for image in images)\n#         targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n\n#         optimizer.zero_grad()\n#         loss_dict = model(images, targets)\n#         losses = sum(loss for loss in loss_dict.values())\n#         loss_value = losses.item()\n\n#         loss_hist.send(loss_value)\n\n#         losses.backward()\n#         optimizer.step()\n\n#         if itr % PRINT_STEP == 0:\n#             print(f\"Iteration #{itr} loss: {loss_value}\")\n\n#         itr += 1\n\n#     epoch_avg_loss = loss_hist.value\n#     epoch_losses.append(epoch_avg_loss)\n#     print(f\"Epoch #{epoch} average loss: {epoch_avg_loss}\")\n    \n#     # update the learning rate\n#     if lr_scheduler is not None:\n#         lr_scheduler.step()\n        \n#     model_save_path = f'model_epoch_{epoch+1}.pth'\n#     torch.save(model.state_dict(), model_save_path)\n#     print(f\"Model saved to {model_save_path}\")\n        \n        \n# plt.figure(figsize=(10, 5))  \n# plt.plot(epoch_losses, color='blue')  \n# plt.title('Training Loss')  \n# plt.xlabel('Epoch')  \n# plt.ylabel('Average Loss')  \n# plt.show()  ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# torch.save(model.state_dict(), 'cats_dogs_detection.pth')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from torchvision.ops import nms\n\ndef calculate_iou(box1, box2):\n    \"\"\"Calculate Intersection over Union (IoU) for two given boxes.\"\"\"\n    # Coordinates of the intersection box\n    x1 = max(box1[0], box2[0])\n    y1 = max(box1[1], box2[1])\n    x2 = min(box1[2], box2[2])\n    y2 = min(box1[3], box2[3])\n\n    # Area of intersection\n    inter_area = max(0, x2 - x1) * max(0, y2 - y1)\n\n    # Area of both boxes\n    box1_area = (box1[2] - box1[0]) * (box1[3] - box1[1])\n    box2_area = (box2[2] - box2[0]) * (box2[3] - box2[1])\n\n    # Area of union\n    union_area = box1_area + box2_area - inter_area\n\n    # Compute IoU\n    iou = inter_area / union_area if union_area != 0 else 0\n    return iou\n\ndef apply_nms_and_score_threshold(outputs, score_thresh=0.85, nms_thresh=0.8):\n    \"\"\"Apply score threshold and NMS on model outputs.\"\"\"\n    processed_outputs = []\n    for output in outputs:\n        mask = output['scores'] > score_thresh\n        filtered_boxes = output['boxes'][mask]\n        filtered_scores = output['scores'][mask]\n        filtered_labels = output['labels'][mask]\n\n        keep = nms(filtered_boxes, filtered_scores, nms_thresh)\n\n        nms_boxes = filtered_boxes[keep]\n        nms_scores = filtered_scores[keep]\n        nms_labels = filtered_labels[keep]\n\n        processed_outputs.append({\n            'boxes': nms_boxes,\n            'scores': nms_scores,\n            'labels': nms_labels\n        })\n    return processed_outputs\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# def calculate_accuracy_metrics(processed_outputs, targets):\n#     true_positives = 0\n#     false_positives = 0\n#     false_negatives = 0\n\n#     for output, target in zip(processed_outputs, targets):\n#         target_boxes = target['boxes'].cpu()\n#         target_labels = target['labels'].cpu()\n#         output_boxes = output['boxes'].cpu()\n#         output_labels = output['labels'].cpu()\n\n#         detected = [False] * len(output_boxes)\n\n#         for target_box, target_label in zip(target_boxes, target_labels):\n#             if len(output_boxes) == 0:\n#                 # Dacă nu există detectări, toate sunt false negatives\n#                 false_negatives += len(target_boxes)\n#                 break\n#             ious = torch.tensor([calculate_iou(target_box, ob) for ob in output_boxes])\n#             max_iou, max_index = ious.max(0)\n            \n#             if max_iou > 0.5 and target_label == output_labels[max_index]:\n#                 true_positives += 1\n#                 detected[max_index] = True\n#             else:\n#                 false_negatives += 1\n\n#         false_positives += sum(not d for d in detected)\n\n#     precision = true_positives / (true_positives + false_positives) if true_positives + false_positives > 0 else 0\n#     recall = true_positives / (true_positives + false_negatives) if true_positives + false_negatives > 0 else 0\n#     f1_score = 2 * precision * recall / (precision + recall) if precision + recall > 0 else 0\n\n#     return {'precision': precision, 'recall': recall, 'f1_score': f1_score}\n\ndef calculate_accuracy_metrics(processed_outputs, targets):\n    true_positives = 0\n    false_positives = 0\n    false_negatives = 0\n\n    for output, target in zip(processed_outputs, targets):\n        target_boxes = target['boxes'].cpu()\n        target_labels = target['labels'].cpu()\n        output_boxes = output['boxes'].cpu()\n        output_labels = output['labels'].cpu()\n\n        detected = [False] * len(output_boxes)\n\n        if len(output_boxes) == 0:\n            # Dacă nu există detectări, toate targeturile sunt false negatives\n            false_negatives += len(target_boxes)\n            continue  # Sari la următorul batch\n\n        for target_box, target_label in zip(target_boxes, target_labels):\n            # Calculăm IoU pentru cutia target cu toate detectările\n            ious = torch.tensor([calculate_iou(target_box, ob) for ob in output_boxes])\n            if ious.numel() == 0:\n                # Dacă nu există IoU calculat, continuăm la următoarea cutie target\n                false_negatives += 1\n                continue\n            \n            max_iou, max_index = ious.max(0)\n            if max_iou > 0.5 and target_label == output_labels[max_index]:\n                # True positive: detectare corectă a clasei și IoU suficient\n                true_positives += 1\n                detected[max_index] = True\n            else:\n                # False negative: nicio detectare suficientă pentru această cutie target\n                false_negatives += 1\n\n        # False positives: detectările care nu au fost asociate cu niciun target\n        false_positives += sum(not d for d in detected)\n\n    precision = true_positives / (true_positives + false_positives) if true_positives + false_positives > 0 else 0\n    recall = true_positives / (true_positives + false_negatives) if true_positives + false_negatives > 0 else 0\n    f1_score = 2 * precision * recall / (precision + recall) if precision + recall > 0 else 0\n\n    return {'precision': precision, 'recall': recall, 'f1_score': f1_score}\n\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\ntorch.cuda.empty_cache()\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = create_custom_model(3)\nmodel.load_state_dict(torch.load('/kaggle/input/modele-epoci/model_epoch_1 (1).pth'))\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nimport torchmetrics\n\n# Initialize the accuracy metric\n\nmodel.to(device)\nmodel.eval()  # Set model to training mode to calculate loss\n\nnum_images_to_process = 10\nimages_processed = 0\n\nall_metrics = []\nwith torch.no_grad():\n    for images, targets in valid_data_loader:\n        images = list(img.to(device) for img in images)\n        targets = [{k: v.to(device) if isinstance(v, torch.Tensor) else v for k, v in t.items()} for t in targets]\n\n        outputs = model(images)  # Presupunem că modelul returnează ieșiri brute\n        processed_outputs = apply_nms_and_score_threshold(outputs)\n\n        # Calculăm metricile\n        metrics = calculate_accuracy_metrics(processed_outputs, targets)\n        all_metrics.append(metrics)\n\n# Calculăm media metricilor peste toate batch-urile\nmean_metrics = {k: sum(m[k] for m in all_metrics) / len(all_metrics) for k in all_metrics[0]}\nprint(\"Average Precision:\", mean_metrics['precision'])\nprint(\"Average Recall:\", mean_metrics['recall'])\nprint(\"Average F1 Score:\", mean_metrics['f1_score'])\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = create_custom_model(3)\nmodel.load_state_dict(torch.load('/kaggle/input/modele-epoci/model_epoch_2.pth'))\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nimport torchmetrics\n\n# Initialize the accuracy metric\n\nmodel.to(device)\nmodel.eval()  # Set model to training mode to calculate loss\n\nnum_images_to_process = 10\nimages_processed = 0\n\nall_metrics = []\nwith torch.no_grad():\n    for images, targets in valid_data_loader:\n        images = list(img.to(device) for img in images)\n        targets = [{k: v.to(device) if isinstance(v, torch.Tensor) else v for k, v in t.items()} for t in targets]\n\n        outputs = model(images)  # Presupunem că modelul returnează ieșiri brute\n        processed_outputs = apply_nms_and_score_threshold(outputs)\n\n        # Calculăm metricile\n        metrics = calculate_accuracy_metrics(processed_outputs, targets)\n        all_metrics.append(metrics)\n\n# Calculăm media metricilor peste toate batch-urile\nmean_metrics = {k: sum(m[k] for m in all_metrics) / len(all_metrics) for k in all_metrics[0]}\nprint(\"Average Precision:\", mean_metrics['precision'])\nprint(\"Average Recall:\", mean_metrics['recall'])\nprint(\"Average F1 Score:\", mean_metrics['f1_score'])\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = create_custom_model(3)\nmodel.load_state_dict(torch.load('/kaggle/input/modele-epoci/model_epoch_3.pth'))\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nimport torchmetrics\n\n# Initialize the accuracy metric\n\nmodel.to(device)\nmodel.eval()  # Set model to training mode to calculate loss\n\nnum_images_to_process = 10\nimages_processed = 0\n\nall_metrics = []\nwith torch.no_grad():\n    for images, targets in valid_data_loader:\n        images = list(img.to(device) for img in images)\n        targets = [{k: v.to(device) if isinstance(v, torch.Tensor) else v for k, v in t.items()} for t in targets]\n\n        outputs = model(images)  # Presupunem că modelul returnează ieșiri brute\n        processed_outputs = apply_nms_and_score_threshold(outputs)\n\n        # Calculăm metricile\n        metrics = calculate_accuracy_metrics(processed_outputs, targets)\n        all_metrics.append(metrics)\n\n# Calculăm media metricilor peste toate batch-urile\nmean_metrics = {k: sum(m[k] for m in all_metrics) / len(all_metrics) for k in all_metrics[0]}\nprint(\"Average Precision:\", mean_metrics['precision'])\nprint(\"Average Recall:\", mean_metrics['recall'])\nprint(\"Average F1 Score:\", mean_metrics['f1_score'])\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = create_custom_model(3)\nmodel.load_state_dict(torch.load('/kaggle/input/modele-epoci/model_epoch_4.pth'))\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nimport torchmetrics\n\n# Initialize the accuracy metric\n\nmodel.to(device)\nmodel.eval()  # Set model to training mode to calculate loss\n\nnum_images_to_process = 10\nimages_processed = 0\n\nall_metrics = []\nwith torch.no_grad():\n    for images, targets in valid_data_loader:\n        images = list(img.to(device) for img in images)\n        targets = [{k: v.to(device) if isinstance(v, torch.Tensor) else v for k, v in t.items()} for t in targets]\n\n        outputs = model(images)  # Presupunem că modelul returnează ieșiri brute\n        processed_outputs = apply_nms_and_score_threshold(outputs)\n\n        # Calculăm metricile\n        metrics = calculate_accuracy_metrics(processed_outputs, targets)\n        all_metrics.append(metrics)\n\n# Calculăm media metricilor peste toate batch-urile\nmean_metrics = {k: sum(m[k] for m in all_metrics) / len(all_metrics) for k in all_metrics[0]}\nprint(\"Average Precision:\", mean_metrics['precision'])\nprint(\"Average Recall:\", mean_metrics['recall'])\nprint(\"Average F1 Score:\", mean_metrics['f1_score'])\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = create_custom_model(3)\nmodel.load_state_dict(torch.load('/kaggle/input/modele-epoci/model_epoch_5.pth'))\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nimport torchmetrics\n\n# Initialize the accuracy metric\n\nmodel.to(device)\nmodel.eval()  # Set model to training mode to calculate loss\n\nnum_images_to_process = 10\nimages_processed = 0\n\nall_metrics = []\nwith torch.no_grad():\n    for images, targets in valid_data_loader:\n        images = list(img.to(device) for img in images)\n        targets = [{k: v.to(device) if isinstance(v, torch.Tensor) else v for k, v in t.items()} for t in targets]\n\n        outputs = model(images)  # Presupunem că modelul returnează ieșiri brute\n        processed_outputs = apply_nms_and_score_threshold(outputs)\n\n        # Calculăm metricile\n        metrics = calculate_accuracy_metrics(processed_outputs, targets)\n        all_metrics.append(metrics)\n\n# Calculăm media metricilor peste toate batch-urile\nmean_metrics = {k: sum(m[k] for m in all_metrics) / len(all_metrics) for k in all_metrics[0]}\nprint(\"Average Precision:\", mean_metrics['precision'])\nprint(\"Average Recall:\", mean_metrics['recall'])\nprint(\"Average F1 Score:\", mean_metrics['f1_score'])\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = create_custom_model(3)\nmodel.load_state_dict(torch.load('/kaggle/input/modele-epoci/model_epoch_6.pth'))\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nimport torchmetrics\n\n# Initialize the accuracy metric\n\nmodel.to(device)\nmodel.eval()  # Set model to training mode to calculate loss\n\nnum_images_to_process = 10\nimages_processed = 0\n\nall_metrics = []\nwith torch.no_grad():\n    for images, targets in valid_data_loader:\n        images = list(img.to(device) for img in images)\n        targets = [{k: v.to(device) if isinstance(v, torch.Tensor) else v for k, v in t.items()} for t in targets]\n\n        outputs = model(images)  # Presupunem că modelul returnează ieșiri brute\n        processed_outputs = apply_nms_and_score_threshold(outputs)\n\n        # Calculăm metricile\n        metrics = calculate_accuracy_metrics(processed_outputs, targets)\n        all_metrics.append(metrics)\n\n# Calculăm media metricilor peste toate batch-urile\nmean_metrics = {k: sum(m[k] for m in all_metrics) / len(all_metrics) for k in all_metrics[0]}\nprint(\"Average Precision:\", mean_metrics['precision'])\nprint(\"Average Recall:\", mean_metrics['recall'])\nprint(\"Average F1 Score:\", mean_metrics['f1_score'])\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = create_custom_model(3)\nmodel.load_state_dict(torch.load('/kaggle/input/modele-epoci/model_epoch_7.pth'))\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nimport torchmetrics\n\n# Initialize the accuracy metric\n\nmodel.to(device)\nmodel.eval()  # Set model to training mode to calculate loss\n\nnum_images_to_process = 10\nimages_processed = 0\n\nall_metrics = []\nwith torch.no_grad():\n    for images, targets in valid_data_loader:\n        images = list(img.to(device) for img in images)\n        targets = [{k: v.to(device) if isinstance(v, torch.Tensor) else v for k, v in t.items()} for t in targets]\n\n        outputs = model(images)  # Presupunem că modelul returnează ieșiri brute\n        processed_outputs = apply_nms_and_score_threshold(outputs)\n\n        # Calculăm metricile\n        metrics = calculate_accuracy_metrics(processed_outputs, targets)\n        all_metrics.append(metrics)\n\n# Calculăm media metricilor peste toate batch-urile\nmean_metrics = {k: sum(m[k] for m in all_metrics) / len(all_metrics) for k in all_metrics[0]}\nprint(\"Average Precision:\", mean_metrics['precision'])\nprint(\"Average Recall:\", mean_metrics['recall'])\nprint(\"Average F1 Score:\", mean_metrics['f1_score'])\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = create_custom_model(3)\nmodel.load_state_dict(torch.load('/kaggle/input/modele-epoci/model_epoch_8.pth'))\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nimport torchmetrics\n\n# Initialize the accuracy metric\n\nmodel.to(device)\nmodel.eval()  # Set model to training mode to calculate loss\n\nnum_images_to_process = 10\nimages_processed = 0\n\nall_metrics = []\nwith torch.no_grad():\n    for images, targets in valid_data_loader:\n        images = list(img.to(device) for img in images)\n        targets = [{k: v.to(device) if isinstance(v, torch.Tensor) else v for k, v in t.items()} for t in targets]\n\n        outputs = model(images)  # Presupunem că modelul returnează ieșiri brute\n        processed_outputs = apply_nms_and_score_threshold(outputs)\n\n        # Calculăm metricile\n        metrics = calculate_accuracy_metrics(processed_outputs, targets)\n        all_metrics.append(metrics)\n\n# Calculăm media metricilor peste toate batch-urile\nmean_metrics = {k: sum(m[k] for m in all_metrics) / len(all_metrics) for k in all_metrics[0]}\nprint(\"Average Precision:\", mean_metrics['precision'])\nprint(\"Average Recall:\", mean_metrics['recall'])\nprint(\"Average F1 Score:\", mean_metrics['f1_score'])\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = create_custom_model(3)\nmodel.load_state_dict(torch.load('/kaggle/input/modele-epoci/model_epoch_9.pth'))\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nimport torchmetrics\n\n# Initialize the accuracy metric\n\nmodel.to(device)\nmodel.eval()  # Set model to training mode to calculate loss\n\nnum_images_to_process = 10\nimages_processed = 0\n\nall_metrics = []\nwith torch.no_grad():\n    for images, targets in valid_data_loader:\n        images = list(img.to(device) for img in images)\n        targets = [{k: v.to(device) if isinstance(v, torch.Tensor) else v for k, v in t.items()} for t in targets]\n\n        outputs = model(images)  # Presupunem că modelul returnează ieșiri brute\n        processed_outputs = apply_nms_and_score_threshold(outputs)\n\n        # Calculăm metricile\n        metrics = calculate_accuracy_metrics(processed_outputs, targets)\n        all_metrics.append(metrics)\n\n# Calculăm media metricilor peste toate batch-urile\nmean_metrics = {k: sum(m[k] for m in all_metrics) / len(all_metrics) for k in all_metrics[0]}\nprint(\"Average Precision:\", mean_metrics['precision'])\nprint(\"Average Recall:\", mean_metrics['recall'])\nprint(\"Average F1 Score:\", mean_metrics['f1_score'])\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = create_custom_model(3)\nmodel.load_state_dict(torch.load('/kaggle/input/modele-epoci/model_epoch_10.pth'))\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nimport torchmetrics\n\n# Initialize the accuracy metric\n\nmodel.to(device)\nmodel.eval()  # Set model to training mode to calculate loss\n\nnum_images_to_process = 10\nimages_processed = 0\n\nall_metrics = []\nwith torch.no_grad():\n    for images, targets in valid_data_loader:\n        images = list(img.to(device) for img in images)\n        targets = [{k: v.to(device) if isinstance(v, torch.Tensor) else v for k, v in t.items()} for t in targets]\n\n        outputs = model(images)  # Presupunem că modelul returnează ieșiri brute\n        processed_outputs = apply_nms_and_score_threshold(outputs)\n\n        # Calculăm metricile\n        metrics = calculate_accuracy_metrics(processed_outputs, targets)\n        all_metrics.append(metrics)\n\n# Calculăm media metricilor peste toate batch-urile\nmean_metrics = {k: sum(m[k] for m in all_metrics) / len(all_metrics) for k in all_metrics[0]}\nprint(\"Average Precision:\", mean_metrics['precision'])\nprint(\"Average Recall:\", mean_metrics['recall'])\nprint(\"Average F1 Score:\", mean_metrics['f1_score'])\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Test**","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport matplotlib.patches as patches\nimport numpy as np\n\n# Set the model to evaluation mode\nmodel.to(device)  # where device is either 'cuda' or 'cpu'\nmodel.eval()\n\n\n# Select a few images to display\nnum_images_to_display = 15\nimages_processed = 0\n\nwith torch.no_grad():  # Turn off gradients to speed up this part\n    for images, targets in valid_data_loader:\n        images = list(img.to(device) for img in images)\n        outputs = model(images)  # Forward pass\n\n        for i in range(len(images)):\n            # Move image data to CPU for visualization\n            image_np = images[i].permute(1, 2, 0).cpu().numpy()\n            image_np = np.clip(image_np, 0, 1)  # Ensure image has correct format and range\n\n            fig, ax = plt.subplots(1)\n            ax.imshow(image_np)\n\n            # Get the predicted boxes, labels, and scores\n            pred_boxes = outputs[i]['boxes'].cpu().numpy()\n            pred_labels = outputs[i]['labels'].cpu().numpy()\n            pred_scores = outputs[i]['scores'].cpu().numpy()\n\n            # Draw each box on the image along with the label\n            for box, label, score in zip(pred_boxes, pred_labels, pred_scores):\n                if score > 0.5:  # Threshold to filter out lower score boxes\n                    rect = patches.Rectangle((box[0], box[1]), box[2] - box[0], box[3] - box[1], linewidth=1, edgecolor='r', facecolor='none')\n                    ax.add_patch(rect)\n                    ax.text(box[0], box[1], f'{label} ({score:.2f})', bbox=dict(facecolor='white', alpha=0.5))\n\n            plt.axis('off')\n            plt.show()\n\n            images_processed += 1\n            if images_processed >= num_images_to_display:\n                break\n        if images_processed >= num_images_to_display:\n            break\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Visualization","metadata":{}},{"cell_type":"code","source":"images, targets = next(iter(valid_data_loader))\n\nimages = list(img.to(device) for img in images)\ntargets = [{k: v.to(device) for k, v in t.items()} for t in targets]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Example\nimg_index = 1\n\nsample = images[1].permute(1,2,0).cpu().numpy()\nboxes = targets[1]['boxes'].cpu().numpy().astype(np.int32)\nlabels = targets[1]['labels'].cpu().numpy().astype(np.int32)\n\ndisplay_boxes(sample, boxes, labels)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}