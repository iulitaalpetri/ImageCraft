{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":2415961,"sourceType":"datasetVersion","datasetId":1365967},{"sourceId":8485674,"sourceType":"datasetVersion","datasetId":5061928},{"sourceId":8841903,"sourceType":"datasetVersion","datasetId":5321523}],"dockerImageVersionId":30461,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"raw","source":"import os\nimport cv2\nimport time\nimport random\nimport numpy as np\n\nimport tensorflow as tf\nfrom tensorflow.keras.applications.inception_v3 import preprocess_input\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nfrom tensorflow.keras import backend, layers, metrics\n\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.applications import Xception\nfrom tensorflow.keras.models import Model, Sequential\n\nfrom tensorflow.keras.utils import plot_model\nfrom sklearn.metrics import accuracy_score, confusion_matrix, classification_report","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-05-18T22:46:39.000805Z","iopub.execute_input":"2024-05-18T22:46:39.002177Z","iopub.status.idle":"2024-05-18T22:46:39.009664Z","shell.execute_reply.started":"2024-05-18T22:46:39.002120Z","shell.execute_reply":"2024-05-18T22:46:39.008569Z"}}},{"cell_type":"code","source":"import torch \n\ndevice = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n","metadata":{"execution":{"iopub.status.busy":"2024-05-18T22:46:52.626909Z","iopub.execute_input":"2024-05-18T22:46:52.627309Z","iopub.status.idle":"2024-05-18T22:46:54.673851Z","shell.execute_reply.started":"2024-05-18T22:46:52.627273Z","shell.execute_reply":"2024-05-18T22:46:54.672716Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset = \"/kaggle/input/face-recognition-dataset/Extracted Faces/Extracted Faces\"\n\ndef read_image(index):\n    path = os.path.join(dataset, index[0], index[1])\n    image = cv2.imread(path)\n    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n    return image","metadata":{"execution":{"iopub.status.busy":"2024-05-18T22:47:04.384988Z","iopub.execute_input":"2024-05-18T22:47:04.385402Z","iopub.status.idle":"2024-05-18T22:47:04.391461Z","shell.execute_reply.started":"2024-05-18T22:47:04.385363Z","shell.execute_reply":"2024-05-18T22:47:04.390451Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport random\n\ndef split_dataset(path, split=0.9):\n    \n    folders = os.listdir(path)\n    \n    num_train = int(len(folders) * split)\n    \n    random.shuffle(folders)\n    \n    train_list, test_list = {}, {}\n    \n    for folder in folders[:num_train]:\n        folder_path = os.path.join(path, folder)\n        num_files = len(os.listdir(folder_path))\n        train_list[folder] = num_files\n       \n    for folder in folders[num_train:]:\n        folder_path = os.path.join(path, folder)\n        num_files = len(os.listdir(folder_path))\n        test_list[folder] = num_files\n        \n    \n    return train_list, test_list\n\n\ntrain_list, test_list = split_dataset(dataset, split=0.90)","metadata":{"execution":{"iopub.status.busy":"2024-05-18T22:47:13.817381Z","iopub.execute_input":"2024-05-18T22:47:13.818375Z","iopub.status.idle":"2024-05-18T22:47:24.244549Z","shell.execute_reply.started":"2024-05-18T22:47:13.818335Z","shell.execute_reply":"2024-05-18T22:47:24.243626Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\ndef create_triplets(directory, folder_list, max_files=10):\n    \n    triplets = []\n    folders = list(folder_list.keys())\n    \n    for folder in folders:\n        path = os.path.join(directory, folder)\n        files = os.listdir(path)\n        random.shuffle(files)  \n        files = files[:max_files]  \n        num_files = len(files)\n        \n        for i in range(num_files - 1):\n            for j in range(i + 1, num_files):\n                anchor = (folder, files[i])\n                positive = (folder, files[j])\n\n                neg_folder = random.choice([f for f in folders if f != folder])\n                neg_file = random.choice(os.listdir(os.path.join(directory, neg_folder)))\n                \n                negative = (neg_folder, neg_file)\n                triplets.append((anchor, positive, negative))\n    \n    random.shuffle(triplets)  \n    return triplets\n\n","metadata":{"execution":{"iopub.status.busy":"2024-05-18T22:48:30.291614Z","iopub.execute_input":"2024-05-18T22:48:30.292579Z","iopub.status.idle":"2024-05-18T22:48:30.302325Z","shell.execute_reply.started":"2024-05-18T22:48:30.292539Z","shell.execute_reply":"2024-05-18T22:48:30.301199Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_triplet = create_triplets(dataset, train_list)\ntest_triplet  = create_triplets(dataset, test_list)","metadata":{"execution":{"iopub.status.busy":"2024-05-10T12:37:46.356398Z","iopub.execute_input":"2024-05-10T12:37:46.357179Z","iopub.status.idle":"2024-05-10T12:37:55.802406Z","shell.execute_reply.started":"2024-05-10T12:37:46.357135Z","shell.execute_reply":"2024-05-10T12:37:55.801245Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_batch(triplet_list, batch_size=100, preprocess=True):\n   \n    batch_steps = len(triplet_list) // batch_size\n\n    for i in range(batch_steps + 1):\n        anchor = []\n        positive = []\n        negative = []\n        \n        start_index = i * batch_size\n        end_index = min((i + 1) * batch_size, len(triplet_list))\n\n        for j in range(start_index, end_index):\n            a, p, n = triplet_list[j]\n            anchor.append(read_image(a))\n            positive.append(read_image(p))\n            negative.append(read_image(n))\n        \n        anchor = np.array(anchor)\n        positive = np.array(positive)\n        negative = np.array(negative)\n        \n        if preprocess:\n            anchor = preprocess_input(anchor)\n            positive = preprocess_input(positive)\n            negative = preprocess_input(negative)\n        \n        yield ([anchor, positive, negative])","metadata":{"execution":{"iopub.status.busy":"2024-05-10T12:38:03.441225Z","iopub.execute_input":"2024-05-10T12:38:03.442456Z","iopub.status.idle":"2024-05-10T12:38:03.452224Z","shell.execute_reply.started":"2024-05-10T12:38:03.442406Z","shell.execute_reply":"2024-05-10T12:38:03.450935Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_encoder(input_shape):\n  \n    pretrained_model = Xception(\n        input_shape=input_shape,\n        weights='imagenet',\n        include_top=False,\n        pooling='avg',\n    )\n    \n    for layer in pretrained_model.layers[:-27]:\n        layer.trainable = False\n\n    encode_model = Sequential([\n        pretrained_model,\n        layers.Flatten(),\n        layers.Dense(512, activation='relu'),\n        layers.BatchNormalization(),\n        layers.Dense(256, activation=\"relu\"),\n        layers.Lambda(lambda x: tf.math.l2_normalize(x, axis=1))\n    ], name=\"Encode_Model\")\n    \n    return encode_model","metadata":{"execution":{"iopub.status.busy":"2024-05-10T12:38:05.907061Z","iopub.execute_input":"2024-05-10T12:38:05.907984Z","iopub.status.idle":"2024-05-10T12:38:05.916577Z","shell.execute_reply.started":"2024-05-10T12:38:05.907939Z","shell.execute_reply":"2024-05-10T12:38:05.915101Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class DistanceLayer(layers.Layer):\n    # A layer to compute ‖f(A) - f(P)‖² and ‖f(A) - f(N)‖²\n    def __init__(self, **kwargs):\n        super().__init__(**kwargs)\n\n    def call(self, anchor, positive, negative):\n        ap_distance = tf.reduce_sum(tf.square(anchor - positive), -1)\n        an_distance = tf.reduce_sum(tf.square(anchor - negative), -1)\n        return (ap_distance, an_distance)\n    \n\ndef get_siamese_network(input_shape = (128, 128, 3)):\n    encoder = get_encoder(input_shape)\n    \n    # Input Layers for the images\n    anchor_input   = layers.Input(input_shape, name=\"Anchor_Input\")\n    positive_input = layers.Input(input_shape, name=\"Positive_Input\")\n    negative_input = layers.Input(input_shape, name=\"Negative_Input\")\n    \n    encoded_a = encoder(anchor_input)\n    encoded_p = encoder(positive_input)\n    encoded_n = encoder(negative_input)\n    \n    distances = DistanceLayer()(\n        encoder(anchor_input),\n        encoder(positive_input),\n        encoder(negative_input)\n    )\n    \n    siamese_network = Model(\n        inputs  = [anchor_input, positive_input, negative_input],\n        outputs = distances,\n        name = \"Siamese_Network\"\n    )\n    return siamese_network\n\nsiamese_network = get_siamese_network()\nsiamese_network.summary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_model(siamese_network, show_shapes=True, show_layer_names=True)","metadata":{"execution":{"iopub.status.busy":"2024-05-10T12:38:23.022117Z","iopub.execute_input":"2024-05-10T12:38:23.022539Z","iopub.status.idle":"2024-05-10T12:38:23.275872Z","shell.execute_reply.started":"2024-05-10T12:38:23.022504Z","shell.execute_reply":"2024-05-10T12:38:23.274106Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class SiameseModel(Model):\n    # Builds a Siamese model based on a base-model\n    def __init__(self, siamese_network, margin=4):\n        super(SiameseModel, self).__init__()\n        \n        # Marginea folosită în calculul pierderii triplet\n        self.margin = margin\n        \n        # Rețeaua siameză pe care o vom antrena\n        self.siamese_network = siamese_network\n        \n        # Un metric pentru a urmări pierderea medie în timpul antrenamentului\n        self.loss_tracker = metrics.Mean(name=\"loss\")\n\n    # Metoda call definește comportamentul modelului la efectuarea unei treceri înainte\n    def call(self, inputs):\n        return self.siamese_network(inputs)\n\n    # Metoda train_step definește un singur pas de antrenament\n    def train_step(self, data):\n        # GradientTape înregistrează toate operațiile pentru a calcula gradientii.\n        with tf.GradientTape() as tape:\n            # Calculăm pierderea folosind datele de antrenament.\n            loss = self._compute_loss(data)\n            \n        # Calculăm gradientii pierderii față de toate greutățile antrenabile ale rețelei.\n        gradients = tape.gradient(loss, self.siamese_network.trainable_weights)\n        \n        # Aplicăm gradientii la greutăți pentru a le actualiza și pentru a reduce pierderea.\n        self.optimizer.apply_gradients(zip(gradients, self.siamese_network.trainable_weights))\n        \n        # Actualizăm starea pierderii pentru a ține evidența pierderii medii.\n        self.loss_tracker.update_state(loss)\n        \n        # Returnăm pierderea medie pentru a monitoriza performanța modelului în timpul antrenamentului.\n        return {\"loss\": self.loss_tracker.result()}\n\n    # Metoda test_step definește un singur pas de testare/validare\n    def test_step(self, data):\n        # Calculăm pierderea folosind datele de testare.\n        loss = self._compute_loss(data)\n        \n        # Actualizăm starea pierderii pentru a ține evidența pierderii medii.\n        self.loss_tracker.update_state(loss)\n        \n        # Returnăm pierderea medie pentru a monitoriza performanța modelului în timpul testării/validării.\n        return {\"loss\": self.loss_tracker.result()}\n\n    # Metoda _compute_loss definește modul de calcul al pierderii\n    def _compute_loss(self, data):\n        # Obținem cele două distanțe de la rețeaua siameză: ancoră-pozitiv și ancoră-negativ\n        ap_distance, an_distance = self.siamese_network(data)\n        \n        # Calculăm pierderea triplet folosind distanțele și marginea\n        loss = tf.maximum(ap_distance - an_distance + self.margin, 0.0)\n        \n        # Returnăm pierderea\n        return loss\n\n    @property\n    def metrics(self):\n        # Listăm metricile noastre pentru ca reset_states() să fie apelat automat.\n        return [self.loss_tracker]\n","metadata":{"execution":{"iopub.status.busy":"2024-05-10T12:40:00.216617Z","iopub.execute_input":"2024-05-10T12:40:00.217158Z","iopub.status.idle":"2024-05-10T12:40:00.233532Z","shell.execute_reply.started":"2024-05-10T12:40:00.217111Z","shell.execute_reply":"2024-05-10T12:40:00.231626Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"siamese_model = SiameseModel(siamese_network)\noptimizer = Adam(learning_rate=1e-3, epsilon=1e-01)\nsiamese_model.compile(optimizer=optimizer)","metadata":{"execution":{"iopub.status.busy":"2024-05-10T12:40:12.508730Z","iopub.execute_input":"2024-05-10T12:40:12.509208Z","iopub.status.idle":"2024-05-10T12:40:12.547587Z","shell.execute_reply.started":"2024-05-10T12:40:12.509166Z","shell.execute_reply":"2024-05-10T12:40:12.546279Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def test_on_triplets(data_triplet, batch_size = 256):\n    pos_scores, neg_scores = [], []\n\n    for data in get_batch(data_triplet, batch_size=batch_size):\n        prediction = siamese_model.predict(data)\n        pos_scores += list(prediction[0])\n        neg_scores += list(prediction[1])\n    \n    accuracy = np.sum(np.array(pos_scores) < np.array(neg_scores)) / len(pos_scores)\n    \n    return accuracy","metadata":{"execution":{"iopub.status.busy":"2024-05-10T12:40:15.034099Z","iopub.execute_input":"2024-05-10T12:40:15.034544Z","iopub.status.idle":"2024-05-10T12:40:15.042274Z","shell.execute_reply.started":"2024-05-10T12:40:15.034495Z","shell.execute_reply":"2024-05-10T12:40:15.040586Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"save_all = False\nepochs = 5\nbatch_size = 128\n\nmax_acc = 0\ntrain_loss = []\nval_loss = []\ntrain_acc = []\nval_acc = []\n\nwith tf.device('/gpu:0'):\n    for epoch in range(1, epochs+1):\n        t = time.time()\n\n        # Training the model on train data\n        epoch_train_loss = []\n        for data in get_batch(train_triplet, batch_size=batch_size):\n            loss = siamese_model.train_on_batch(data)\n            epoch_train_loss.append(loss)\n        epoch_train_loss = sum(epoch_train_loss)/len(epoch_train_loss)\n        train_loss.append(epoch_train_loss)\n\n        epoch_val_loss = []\n        for data in get_batch(test_triplet, batch_size=batch_size):\n            loss = siamese_model.train_on_batch(data)\n            epoch_val_loss.append(loss)\n        epoch_val_loss = sum(epoch_val_loss)/len(epoch_val_loss)\n        val_loss.append(epoch_val_loss)\n\n        print(f\"\\nEPOCH: {epoch} \\t (Epoch done in {int(time.time()-t)} sec)\")\n        print(f\"Loss on train : {epoch_train_loss:.5f} | Loss on val : {epoch_val_loss:.5f}\")\n        \n        acc_train = test_on_triplets(train_triplet, batch_size=batch_size)\n        train_acc.append(acc_train)\n        # Testing the model on test data\n        acc_val = test_on_triplets(test_triplet, batch_size=batch_size)\n        val_acc.append(acc_val)\n        accuracy = acc_val\n\n        # Saving the model weights\n        if save_all or accuracy>=max_acc:\n            siamese_model.save_weights(\"siamese_model.h5\")\n            max_acc = accuracy\n            \n        print(f\"Accuracy on train = {acc_train:.5f} | Accuracy on test = {accuracy:.5f}\")\n\n# Saving the model after all epochs run\nsiamese_model.save_weights(\"siamese_model-final.h5\")","metadata":{"execution":{"iopub.status.busy":"2024-05-10T12:40:25.074821Z","iopub.execute_input":"2024-05-10T12:40:25.075255Z","iopub.status.idle":"2024-05-10T13:01:22.426969Z","shell.execute_reply.started":"2024-05-10T12:40:25.075217Z","shell.execute_reply":"2024-05-10T13:01:22.425289Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def plot_metrics(loss_train, loss_val, accuracy):\n    plt.figure(figsize=(15,5))\n    \n    # Plotting the loss over epochs\n    plt.subplot(121)\n    plt.plot(train_loss, 'b', label='Train Loss')\n    plt.plot(val_loss, 'r', label='Val Loss')\n    plt.title('Loss')\n    plt.legend()\n    \n    # Plotting the accuracy over epochs\n    plt.subplot(122)\n    plt.plot(train_acc, 'b', label='Train Accuracy')\n    plt.plot(val_acc, 'r', label='Val Accuracy')\n\n    plt.title('Accuracy')\n    plt.legend()\n    \n    plt.figure(figsize=(15,5))\n\nval_acc = np.array(val_acc)\nplot_metrics(train_loss, val_loss, val_acc)","metadata":{"execution":{"iopub.status.busy":"2024-05-10T08:11:59.584507Z","iopub.status.idle":"2024-05-10T08:11:59.585157Z","shell.execute_reply.started":"2024-05-10T08:11:59.584799Z","shell.execute_reply":"2024-05-10T08:11:59.584835Z"},"trusted":true},"execution_count":null,"outputs":[]}]}